{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python notebook for pre-processing apical image stack.\n",
    "Assumes folder directory structure:\n",
    "<pre><code>  IMAGING\n",
    "    image_stacks\n",
    "    notebooks\n",
    "    results\n",
    "</code></pre>\n",
    "Execute the code sequentially, one block at a time, using &lt;shift-return&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy.interpolate import splprep, splev\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from skimage import color, data, exposure, filters, io\n",
    "from skimage.draw import circle, circle_perimeter\n",
    "from skimage.feature import canny\n",
    "from skimage.morphology import binary_erosion, binary_dilation\n",
    "from skimage.morphology import remove_small_objects\n",
    "from skimage.restoration import denoise_bilateral, denoise_wavelet\n",
    "from skimage.util import img_as_ubyte, img_as_int, img_as_float\n",
    "import skimage.transform as tf\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User editable parameters.\n",
    "Take care with formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "# user editable parameters\n",
    "\n",
    "#image_stack = \"../image_stacks/Mistgcamp-3_0003.tif\"\n",
    "#image_stack = \"../image_stacks/MOVEMENT.tif\"\n",
    "##image_stack = \"../image_stacks/modestmovement.tif\"\n",
    "image_stack = \"Movement2\"\n",
    "#image_stack = \"../image_stacks/lessmovement3.tif\"\n",
    "image_bits = 10\n",
    "\n",
    "# set to either True or False\n",
    "high_magnification = True \n",
    "\n",
    "################################################################################\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the image stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load picture\n",
    "images = io.imread(\"../image_stacks/\" + image_stack + \".tif\")\n",
    "images = np.float32(images/(2.0**image_bits))\n",
    "for i in images:\n",
    "  for l in range(i.shape[0] - 1): # moving average over every two lines\n",
    "    i[l] = (i[l] + i[l+1]) / 2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Use this code block to interactively explore landmark nuclei detection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecab99494cd47d18226d0469f43c3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='image gain', max=5.0, min=1.0), IntRangeSlider(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "@interact(\n",
    "  gn=widgets.FloatSlider(description='image gain',min=1.0, max=5.0, step=0.1, value=1.0),\n",
    "  sr=widgets.IntRangeSlider(description='stack range',min=0, max=300, step=1, value=[0,8]), \n",
    "  bs=widgets.FloatSlider(description='BILATERAL sigma',min=0.0, max=4.0, step=0.1, value=1.0), \n",
    "  cs=widgets.FloatSlider(description='CANNY sigma',min=1.0, max=4.0, step=0.1, value=1.8), \n",
    "  ct=widgets.IntRangeSlider(description='threshold',min=0, max=100, step=1, value=[9,22]),\n",
    "  hr=widgets.IntRangeSlider(description='HOUGH radii',min=3, max=25, step=1, value=[x*(2 if high_magnification else 1) for x in[5,8]]),\n",
    "  hd=widgets.IntSlider(description='distance',min=5, max=50, step=1, value=10),\n",
    "  hp=widgets.IntSlider(description='peaks',min=50, max=500, step=10, value=270),\n",
    "  ht=widgets.FloatSlider(description='threshold',min=0.0, max=1.0, step=0.01, value=0.12),\n",
    "  cr=widgets.FloatSlider(description='circle ratio',min=1.0, max=2.0, step=0.01, value=1.2))\n",
    "\n",
    "def f(gn, sr, bs, cs, ct, hr, hd, hp, ht, cr):\n",
    "  fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(8,8))\n",
    "  A = gn*np.mean(images[sr[0]:sr[1]], axis=0) # the static images\n",
    "  A0 = A / np.amax(A) # normalize\n",
    "  imageA = color.gray2rgb(img_as_ubyte(A0))\n",
    "\n",
    "  # identify nuclei (circles)   \n",
    "  #A = filters.gaussian(A0, sigma=gs) # noise filter\n",
    "  #A = denoise_wavelet(A0, wavelet_levels=7, multichannel=False, rescale_sigma=False)\n",
    "  A = denoise_bilateral(A0, sigma_spatial=bs)\n",
    "  edges = canny(img_as_ubyte(A), sigma=cs, low_threshold=ct[0], high_threshold=ct[1])\n",
    "  hough_radii = np.arange(hr[0], hr[1], 1) # the range of radii to use in search\n",
    "  hough_res = tf.hough_circle(edges, hough_radii) # look for circles\n",
    "  accums, cx, cy, radii = tf.hough_circle_peaks(hough_res, hough_radii, min_xdistance=hd, \n",
    "                                           min_ydistance=hd, total_num_peaks=hp, \n",
    "                                           threshold=ht, normalize=False)\n",
    "\n",
    "  # remove false positives (bright disks with dark perimeter)\n",
    "  pix = [] # as an empty list (for the remaining center pixels)\n",
    "  for center_y, center_x, radius in zip(cy, cx, radii):\n",
    "    c = circle(center_y, center_x, radius, shape=A0.shape) # central disk\n",
    "    cp = circle_perimeter(center_y, center_x, radius+1, shape=A0.shape) # perimeter ring\n",
    "    if (np.mean(imageA[cp]) / np.mean(imageA[c])) > cr:\n",
    "      pix.append((center_x, center_y)) # dark disks with bright perimeter are OK\n",
    "\n",
    "  # remove duplicates (close center pixels)\n",
    "  pix = np.array(pix) # as a numpy array\n",
    "  tree = cKDTree(pix) # for pairwise distance query\n",
    "  rows_to_fuse = list(tree.query_pairs(r=8.0))\n",
    "  p = np.ones(pix.shape[0])           # array of \"keep\" flags\n",
    "  p[np.array(rows_to_fuse)[:,0]] = 0  # flag the first of all duplicate pairs for deletion\n",
    "  pixx = pix[p.astype(bool)]          # the remaining center pixels \n",
    "\n",
    "  # draw nuclei centre pixels\n",
    "  for i in pixx:\n",
    "    #imageA[i[1], i[0]] = (255,0,0)\n",
    "    imageA[circle(i[1], i[0], 1.1, shape=A0.shape)] = (255,0,0)\n",
    "  \n",
    "  ax.imshow(imageA, norm=None)\n",
    "  plt.show()    \n",
    "  return(str(pixx.shape[0]) + \" nuclei identified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all landmark nuclei in the image stack.\n",
    "NOTE: Can take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack index 3 - 28 nuclei identified\n",
      "stack index 4 - 27 nuclei identified\n",
      "stack index 5 - 31 nuclei identified\n",
      "stack index 6 - 28 nuclei identified\n",
      "stack index 7 - 23 nuclei identified\n",
      "stack index 8 - 23 nuclei identified\n",
      "stack index 9 - 23 nuclei identified\n",
      "stack index 10 - 26 nuclei identified\n",
      "stack index 11 - 24 nuclei identified\n",
      "stack index 12 - 29 nuclei identified\n",
      "stack index 13 - 23 nuclei identified\n",
      "stack index 14 - 26 nuclei identified\n",
      "stack index 15 - 27 nuclei identified\n",
      "stack index 16 - 24 nuclei identified\n",
      "stack index 17 - 27 nuclei identified\n",
      "stack index 18 - 30 nuclei identified\n",
      "stack index 19 - 27 nuclei identified\n",
      "stack index 20 - 25 nuclei identified\n",
      "stack index 21 - 26 nuclei identified\n",
      "stack index 22 - 23 nuclei identified\n",
      "stack index 23 - 23 nuclei identified\n",
      "stack index 24 - 27 nuclei identified\n",
      "stack index 25 - 26 nuclei identified\n",
      "stack index 26 - 28 nuclei identified\n",
      "stack index 27 - 27 nuclei identified\n",
      "stack index 28 - 27 nuclei identified\n",
      "stack index 29 - 26 nuclei identified\n",
      "stack index 30 - 25 nuclei identified\n",
      "stack index 31 - 28 nuclei identified\n",
      "stack index 32 - 27 nuclei identified\n",
      "stack index 33 - 27 nuclei identified\n",
      "stack index 34 - 26 nuclei identified\n",
      "stack index 35 - 23 nuclei identified\n",
      "stack index 36 - 26 nuclei identified\n",
      "stack index 37 - 27 nuclei identified\n",
      "stack index 38 - 24 nuclei identified\n",
      "stack index 39 - 29 nuclei identified\n",
      "stack index 40 - 28 nuclei identified\n",
      "stack index 41 - 32 nuclei identified\n",
      "stack index 42 - 28 nuclei identified\n",
      "stack index 43 - 24 nuclei identified\n",
      "stack index 44 - 31 nuclei identified\n",
      "stack index 45 - 27 nuclei identified\n",
      "stack index 46 - 22 nuclei identified\n",
      "stack index 47 - 24 nuclei identified\n",
      "stack index 48 - 25 nuclei identified\n",
      "stack index 49 - 26 nuclei identified\n",
      "stack index 50 - 25 nuclei identified\n",
      "stack index 51 - 30 nuclei identified\n",
      "stack index 52 - 26 nuclei identified\n",
      "stack index 53 - 23 nuclei identified\n",
      "stack index 54 - 23 nuclei identified\n",
      "stack index 55 - 23 nuclei identified\n",
      "stack index 56 - 23 nuclei identified\n",
      "stack index 57 - 25 nuclei identified\n",
      "stack index 58 - 25 nuclei identified\n",
      "stack index 59 - 21 nuclei identified\n",
      "stack index 60 - 22 nuclei identified\n",
      "stack index 61 - 21 nuclei identified\n",
      "stack index 62 - 23 nuclei identified\n",
      "stack index 63 - 23 nuclei identified\n",
      "stack index 64 - 27 nuclei identified\n",
      "stack index 65 - 27 nuclei identified\n",
      "stack index 66 - 24 nuclei identified\n",
      "stack index 67 - 23 nuclei identified\n",
      "stack index 68 - 26 nuclei identified\n",
      "stack index 69 - 27 nuclei identified\n",
      "stack index 70 - 25 nuclei identified\n",
      "stack index 71 - 25 nuclei identified\n",
      "stack index 72 - 26 nuclei identified\n",
      "stack index 73 - 23 nuclei identified\n",
      "stack index 74 - 24 nuclei identified\n",
      "stack index 75 - 26 nuclei identified\n",
      "stack index 76 - 30 nuclei identified\n",
      "stack index 77 - 28 nuclei identified\n",
      "stack index 78 - 26 nuclei identified\n",
      "stack index 79 - 22 nuclei identified\n",
      "stack index 80 - 26 nuclei identified\n",
      "stack index 81 - 30 nuclei identified\n",
      "stack index 82 - 23 nuclei identified\n",
      "stack index 83 - 29 nuclei identified\n",
      "stack index 84 - 26 nuclei identified\n",
      "stack index 85 - 25 nuclei identified\n",
      "stack index 86 - 26 nuclei identified\n",
      "stack index 87 - 26 nuclei identified\n",
      "stack index 88 - 29 nuclei identified\n",
      "stack index 89 - 29 nuclei identified\n",
      "stack index 90 - 27 nuclei identified\n",
      "stack index 91 - 28 nuclei identified\n",
      "stack index 92 - 27 nuclei identified\n",
      "stack index 93 - 26 nuclei identified\n",
      "stack index 94 - 24 nuclei identified\n",
      "stack index 95 - 21 nuclei identified\n",
      "stack index 96 - 25 nuclei identified\n",
      "stack index 97 - 24 nuclei identified\n",
      "stack index 98 - 28 nuclei identified\n",
      "stack index 99 - 28 nuclei identified\n",
      "stack index 100 - 27 nuclei identified\n",
      "stack index 101 - 24 nuclei identified\n",
      "stack index 102 - 26 nuclei identified\n",
      "stack index 103 - 22 nuclei identified\n",
      "stack index 104 - 26 nuclei identified\n",
      "stack index 105 - 26 nuclei identified\n",
      "stack index 106 - 25 nuclei identified\n",
      "stack index 107 - 27 nuclei identified\n",
      "stack index 108 - 26 nuclei identified\n",
      "stack index 109 - 24 nuclei identified\n",
      "stack index 110 - 22 nuclei identified\n",
      "stack index 111 - 21 nuclei identified\n",
      "stack index 112 - 21 nuclei identified\n",
      "stack index 113 - 24 nuclei identified\n",
      "stack index 114 - 25 nuclei identified\n",
      "stack index 115 - 26 nuclei identified\n",
      "stack index 116 - 22 nuclei identified\n",
      "stack index 117 - 27 nuclei identified\n",
      "stack index 118 - 25 nuclei identified\n",
      "stack index 119 - 26 nuclei identified\n",
      "stack index 120 - 24 nuclei identified\n",
      "stack index 121 - 24 nuclei identified\n",
      "stack index 122 - 28 nuclei identified\n",
      "stack index 123 - 28 nuclei identified\n",
      "stack index 124 - 25 nuclei identified\n",
      "stack index 125 - 25 nuclei identified\n",
      "stack index 126 - 23 nuclei identified\n",
      "stack index 127 - 25 nuclei identified\n",
      "stack index 128 - 23 nuclei identified\n",
      "stack index 129 - 26 nuclei identified\n",
      "stack index 130 - 23 nuclei identified\n",
      "stack index 131 - 26 nuclei identified\n",
      "stack index 132 - 24 nuclei identified\n",
      "stack index 133 - 28 nuclei identified\n",
      "stack index 134 - 28 nuclei identified\n",
      "stack index 135 - 28 nuclei identified\n",
      "stack index 136 - 28 nuclei identified\n",
      "stack index 137 - 27 nuclei identified\n",
      "stack index 138 - 23 nuclei identified\n",
      "stack index 139 - 25 nuclei identified\n",
      "stack index 140 - 27 nuclei identified\n",
      "stack index 141 - 27 nuclei identified\n",
      "stack index 142 - 24 nuclei identified\n",
      "stack index 143 - 28 nuclei identified\n",
      "stack index 144 - 26 nuclei identified\n",
      "stack index 145 - 23 nuclei identified\n",
      "stack index 146 - 23 nuclei identified\n",
      "stack index 147 - 26 nuclei identified\n",
      "stack index 148 - 25 nuclei identified\n",
      "stack index 149 - 23 nuclei identified\n",
      "stack index 150 - 23 nuclei identified\n",
      "stack index 151 - 25 nuclei identified\n",
      "stack index 152 - 22 nuclei identified\n",
      "stack index 153 - 25 nuclei identified\n",
      "stack index 154 - 25 nuclei identified\n",
      "stack index 155 - 28 nuclei identified\n",
      "stack index 156 - 27 nuclei identified\n",
      "stack index 157 - 31 nuclei identified\n",
      "stack index 158 - 26 nuclei identified\n",
      "stack index 159 - 28 nuclei identified\n",
      "stack index 160 - 28 nuclei identified\n",
      "stack index 161 - 29 nuclei identified\n",
      "stack index 162 - 30 nuclei identified\n",
      "stack index 163 - 34 nuclei identified\n",
      "stack index 164 - 32 nuclei identified\n",
      "stack index 165 - 32 nuclei identified\n",
      "stack index 166 - 28 nuclei identified\n",
      "stack index 167 - 28 nuclei identified\n",
      "stack index 168 - 25 nuclei identified\n",
      "stack index 169 - 33 nuclei identified\n",
      "stack index 170 - 26 nuclei identified\n",
      "stack index 171 - 33 nuclei identified\n",
      "stack index 172 - 27 nuclei identified\n",
      "stack index 173 - 25 nuclei identified\n",
      "stack index 174 - 27 nuclei identified\n",
      "stack index 175 - 28 nuclei identified\n",
      "stack index 176 - 24 nuclei identified\n",
      "stack index 177 - 22 nuclei identified\n",
      "stack index 178 - 20 nuclei identified\n",
      "stack index 179 - 23 nuclei identified\n",
      "stack index 180 - 21 nuclei identified\n",
      "stack index 181 - 27 nuclei identified\n",
      "stack index 182 - 30 nuclei identified\n",
      "stack index 183 - 24 nuclei identified\n",
      "stack index 184 - 29 nuclei identified\n",
      "stack index 185 - 24 nuclei identified\n",
      "stack index 186 - 26 nuclei identified\n",
      "stack index 187 - 35 nuclei identified\n",
      "stack index 188 - 28 nuclei identified\n",
      "stack index 189 - 26 nuclei identified\n",
      "stack index 190 - 31 nuclei identified\n",
      "stack index 191 - 27 nuclei identified\n",
      "stack index 192 - 27 nuclei identified\n",
      "stack index 193 - 26 nuclei identified\n",
      "stack index 194 - 27 nuclei identified\n",
      "stack index 195 - 27 nuclei identified\n",
      "stack index 196 - 27 nuclei identified\n",
      "stack index 197 - 32 nuclei identified\n",
      "stack index 198 - 28 nuclei identified\n",
      "stack index 199 - 27 nuclei identified\n",
      "stack index 200 - 24 nuclei identified\n",
      "stack index 201 - 20 nuclei identified\n",
      "stack index 202 - 26 nuclei identified\n",
      "stack index 203 - 28 nuclei identified\n",
      "stack index 204 - 24 nuclei identified\n",
      "stack index 205 - 27 nuclei identified\n",
      "stack index 206 - 33 nuclei identified\n",
      "stack index 207 - 31 nuclei identified\n",
      "stack index 208 - 29 nuclei identified\n",
      "stack index 209 - 28 nuclei identified\n",
      "stack index 210 - 28 nuclei identified\n",
      "stack index 211 - 28 nuclei identified\n",
      "stack index 212 - 27 nuclei identified\n",
      "stack index 213 - 30 nuclei identified\n",
      "stack index 214 - 32 nuclei identified\n",
      "stack index 215 - 25 nuclei identified\n",
      "stack index 216 - 30 nuclei identified\n",
      "stack index 217 - 29 nuclei identified\n",
      "stack index 218 - 28 nuclei identified\n",
      "stack index 219 - 26 nuclei identified\n",
      "stack index 220 - 34 nuclei identified\n",
      "stack index 221 - 31 nuclei identified\n",
      "stack index 222 - 32 nuclei identified\n",
      "stack index 223 - 28 nuclei identified\n",
      "stack index 224 - 31 nuclei identified\n",
      "stack index 225 - 29 nuclei identified\n",
      "stack index 226 - 30 nuclei identified\n",
      "stack index 227 - 32 nuclei identified\n",
      "stack index 228 - 34 nuclei identified\n",
      "stack index 229 - 30 nuclei identified\n",
      "stack index 230 - 28 nuclei identified\n",
      "stack index 231 - 29 nuclei identified\n",
      "stack index 232 - 29 nuclei identified\n",
      "stack index 233 - 26 nuclei identified\n",
      "stack index 234 - 38 nuclei identified\n",
      "stack index 235 - 29 nuclei identified\n",
      "stack index 236 - 30 nuclei identified\n",
      "stack index 237 - 31 nuclei identified\n",
      "stack index 238 - 25 nuclei identified\n",
      "stack index 239 - 31 nuclei identified\n",
      "stack index 240 - 32 nuclei identified\n",
      "stack index 241 - 26 nuclei identified\n",
      "stack index 242 - 25 nuclei identified\n",
      "stack index 243 - 24 nuclei identified\n",
      "stack index 244 - 28 nuclei identified\n",
      "stack index 245 - 23 nuclei identified\n",
      "stack index 246 - 26 nuclei identified\n",
      "stack index 247 - 24 nuclei identified\n",
      "stack index 248 - 26 nuclei identified\n",
      "stack index 249 - 28 nuclei identified\n",
      "stack index 250 - 30 nuclei identified\n",
      "stack index 251 - 28 nuclei identified\n",
      "stack index 252 - 27 nuclei identified\n",
      "stack index 253 - 27 nuclei identified\n",
      "stack index 254 - 25 nuclei identified\n",
      "stack index 255 - 30 nuclei identified\n",
      "stack index 256 - 31 nuclei identified\n",
      "stack index 257 - 32 nuclei identified\n",
      "stack index 258 - 31 nuclei identified\n",
      "stack index 259 - 30 nuclei identified\n",
      "stack index 260 - 34 nuclei identified\n",
      "stack index 261 - 35 nuclei identified\n",
      "stack index 262 - 34 nuclei identified\n",
      "stack index 263 - 25 nuclei identified\n",
      "stack index 264 - 29 nuclei identified\n",
      "stack index 265 - 32 nuclei identified\n",
      "stack index 266 - 29 nuclei identified\n",
      "stack index 267 - 31 nuclei identified\n",
      "stack index 268 - 34 nuclei identified\n",
      "stack index 269 - 36 nuclei identified\n",
      "stack index 270 - 32 nuclei identified\n",
      "stack index 271 - 29 nuclei identified\n",
      "stack index 272 - 29 nuclei identified\n",
      "stack index 273 - 29 nuclei identified\n",
      "stack index 274 - 29 nuclei identified\n",
      "stack index 275 - 29 nuclei identified\n",
      "stack index 276 - 27 nuclei identified\n",
      "stack index 277 - 31 nuclei identified\n",
      "stack index 278 - 30 nuclei identified\n",
      "stack index 279 - 32 nuclei identified\n",
      "stack index 280 - 30 nuclei identified\n",
      "stack index 281 - 30 nuclei identified\n",
      "stack index 282 - 31 nuclei identified\n",
      "stack index 283 - 31 nuclei identified\n",
      "stack index 284 - 30 nuclei identified\n",
      "stack index 285 - 29 nuclei identified\n",
      "stack index 286 - 32 nuclei identified\n",
      "stack index 287 - 34 nuclei identified\n",
      "stack index 288 - 34 nuclei identified\n",
      "stack index 289 - 26 nuclei identified\n",
      "stack index 290 - 30 nuclei identified\n",
      "stack index 291 - 27 nuclei identified\n",
      "stack index 292 - 30 nuclei identified\n",
      "stack index 293 - 34 nuclei identified\n",
      "stack index 294 - 29 nuclei identified\n",
      "stack index 295 - 28 nuclei identified\n",
      "stack index 296 - 33 nuclei identified\n",
      "stack index 297 - 31 nuclei identified\n",
      "stack index 298 - 31 nuclei identified\n",
      "stack index 299 - 28 nuclei identified\n",
      "stack index 300 - 32 nuclei identified\n",
      "stack index 301 - 34 nuclei identified\n",
      "stack index 302 - 31 nuclei identified\n",
      "stack index 303 - 34 nuclei identified\n",
      "stack index 304 - 28 nuclei identified\n",
      "stack index 305 - 32 nuclei identified\n",
      "stack index 306 - 35 nuclei identified\n",
      "stack index 307 - 33 nuclei identified\n",
      "stack index 308 - 33 nuclei identified\n",
      "stack index 309 - 33 nuclei identified\n",
      "stack index 310 - 33 nuclei identified\n",
      "stack index 311 - 32 nuclei identified\n",
      "stack index 312 - 29 nuclei identified\n",
      "stack index 313 - 30 nuclei identified\n",
      "stack index 314 - 35 nuclei identified\n",
      "stack index 315 - 32 nuclei identified\n",
      "stack index 316 - 33 nuclei identified\n",
      "stack index 317 - 28 nuclei identified\n",
      "stack index 318 - 34 nuclei identified\n",
      "stack index 319 - 36 nuclei identified\n",
      "stack index 320 - 31 nuclei identified\n",
      "stack index 321 - 32 nuclei identified\n",
      "stack index 322 - 34 nuclei identified\n",
      "stack index 323 - 33 nuclei identified\n",
      "stack index 324 - 33 nuclei identified\n",
      "stack index 325 - 34 nuclei identified\n",
      "stack index 326 - 35 nuclei identified\n",
      "stack index 327 - 32 nuclei identified\n",
      "stack index 328 - 30 nuclei identified\n",
      "stack index 329 - 28 nuclei identified\n",
      "stack index 330 - 31 nuclei identified\n",
      "stack index 331 - 24 nuclei identified\n",
      "stack index 332 - 32 nuclei identified\n",
      "stack index 333 - 30 nuclei identified\n",
      "stack index 334 - 29 nuclei identified\n",
      "stack index 335 - 28 nuclei identified\n",
      "stack index 336 - 28 nuclei identified\n",
      "stack index 337 - 28 nuclei identified\n",
      "stack index 338 - 31 nuclei identified\n",
      "stack index 339 - 35 nuclei identified\n",
      "stack index 340 - 37 nuclei identified\n",
      "stack index 341 - 32 nuclei identified\n",
      "stack index 342 - 26 nuclei identified\n",
      "stack index 343 - 28 nuclei identified\n",
      "stack index 344 - 32 nuclei identified\n",
      "stack index 345 - 33 nuclei identified\n",
      "stack index 346 - 31 nuclei identified\n",
      "stack index 347 - 32 nuclei identified\n",
      "stack index 348 - 37 nuclei identified\n",
      "stack index 349 - 33 nuclei identified\n",
      "stack index 350 - 33 nuclei identified\n",
      "stack index 351 - 34 nuclei identified\n",
      "stack index 352 - 33 nuclei identified\n",
      "stack index 353 - 33 nuclei identified\n",
      "stack index 354 - 34 nuclei identified\n",
      "stack index 355 - 30 nuclei identified\n",
      "stack index 356 - 31 nuclei identified\n",
      "stack index 357 - 27 nuclei identified\n",
      "stack index 358 - 31 nuclei identified\n",
      "stack index 359 - 27 nuclei identified\n",
      "stack index 360 - 30 nuclei identified\n",
      "stack index 361 - 24 nuclei identified\n",
      "stack index 362 - 25 nuclei identified\n",
      "stack index 363 - 24 nuclei identified\n",
      "stack index 364 - 25 nuclei identified\n",
      "stack index 365 - 26 nuclei identified\n",
      "stack index 366 - 30 nuclei identified\n",
      "stack index 367 - 27 nuclei identified\n",
      "stack index 368 - 28 nuclei identified\n",
      "stack index 369 - 28 nuclei identified\n",
      "stack index 370 - 29 nuclei identified\n",
      "stack index 371 - 32 nuclei identified\n",
      "stack index 372 - 33 nuclei identified\n",
      "stack index 373 - 24 nuclei identified\n",
      "stack index 374 - 25 nuclei identified\n",
      "stack index 375 - 26 nuclei identified\n",
      "stack index 376 - 25 nuclei identified\n",
      "stack index 377 - 25 nuclei identified\n",
      "stack index 378 - 29 nuclei identified\n",
      "stack index 379 - 28 nuclei identified\n",
      "stack index 380 - 29 nuclei identified\n",
      "stack index 381 - 29 nuclei identified\n",
      "stack index 382 - 29 nuclei identified\n",
      "stack index 383 - 29 nuclei identified\n",
      "stack index 384 - 29 nuclei identified\n",
      "stack index 385 - 29 nuclei identified\n",
      "stack index 386 - 26 nuclei identified\n",
      "stack index 387 - 26 nuclei identified\n",
      "stack index 388 - 25 nuclei identified\n",
      "stack index 389 - 28 nuclei identified\n",
      "stack index 390 - 32 nuclei identified\n",
      "stack index 391 - 31 nuclei identified\n",
      "stack index 392 - 26 nuclei identified\n",
      "stack index 393 - 28 nuclei identified\n",
      "stack index 394 - 32 nuclei identified\n",
      "stack index 395 - 26 nuclei identified\n",
      "stack index 396 - 22 nuclei identified\n",
      "stack index 397 - 29 nuclei identified\n",
      "stack index 398 - 27 nuclei identified\n",
      "stack index 399 - 28 nuclei identified\n",
      "stack index 400 - 27 nuclei identified\n",
      "stack index 401 - 28 nuclei identified\n",
      "stack index 402 - 24 nuclei identified\n",
      "stack index 403 - 24 nuclei identified\n",
      "stack index 404 - 28 nuclei identified\n",
      "stack index 405 - 28 nuclei identified\n",
      "stack index 406 - 30 nuclei identified\n",
      "stack index 407 - 25 nuclei identified\n",
      "stack index 408 - 26 nuclei identified\n",
      "stack index 409 - 25 nuclei identified\n",
      "stack index 410 - 28 nuclei identified\n",
      "stack index 411 - 25 nuclei identified\n",
      "stack index 412 - 26 nuclei identified\n",
      "stack index 413 - 27 nuclei identified\n",
      "stack index 414 - 29 nuclei identified\n",
      "stack index 415 - 21 nuclei identified\n",
      "stack index 416 - 22 nuclei identified\n",
      "stack index 417 - 29 nuclei identified\n",
      "stack index 418 - 28 nuclei identified\n",
      "stack index 419 - 27 nuclei identified\n",
      "stack index 420 - 27 nuclei identified\n",
      "stack index 421 - 26 nuclei identified\n",
      "stack index 422 - 29 nuclei identified\n",
      "stack index 423 - 25 nuclei identified\n",
      "stack index 424 - 26 nuclei identified\n",
      "stack index 425 - 25 nuclei identified\n",
      "stack index 426 - 25 nuclei identified\n",
      "stack index 427 - 28 nuclei identified\n",
      "stack index 428 - 27 nuclei identified\n",
      "stack index 429 - 24 nuclei identified\n",
      "stack index 430 - 28 nuclei identified\n",
      "stack index 431 - 28 nuclei identified\n",
      "stack index 432 - 29 nuclei identified\n",
      "stack index 433 - 25 nuclei identified\n",
      "stack index 434 - 26 nuclei identified\n",
      "stack index 435 - 26 nuclei identified\n",
      "stack index 436 - 28 nuclei identified\n",
      "stack index 437 - 30 nuclei identified\n",
      "stack index 438 - 26 nuclei identified\n",
      "stack index 439 - 25 nuclei identified\n",
      "stack index 440 - 31 nuclei identified\n",
      "stack index 441 - 26 nuclei identified\n",
      "stack index 442 - 28 nuclei identified\n",
      "stack index 443 - 32 nuclei identified\n",
      "stack index 444 - 28 nuclei identified\n",
      "stack index 445 - 33 nuclei identified\n",
      "stack index 446 - 29 nuclei identified\n"
     ]
    }
   ],
   "source": [
    "# landmark detection paramters\n",
    "bs = 1.0 \n",
    "cs = 1.8\n",
    "ct = [9,22]\n",
    "hr = [x*(2 if high_magnification else 1) for x in[5,8]]\n",
    "hd = 10\n",
    "hp = 270\n",
    "ht = 0.12\n",
    "cr = 1.2\n",
    "\n",
    "pixx = [] # a list of all the landmark nuclei centers\n",
    "\n",
    "for i in range(3,images.shape[0]-3): # use moving average over seven frames\n",
    "  A = np.mean(images[i-3:i+5], axis=0)\n",
    "  A0 = A / np.amax(A) # normalized\n",
    "\n",
    "  # identify nuclei (circles)   \n",
    "  #A = filters.gaussian(A0, sigma=gs) # noise filter\n",
    "  #A = denoise_wavelet(A0, wavelet_levels=7, multichannel=False, rescale_sigma=False)\n",
    "  A = denoise_bilateral(A0, sigma_spatial=bs)\n",
    "  edges = canny(img_as_ubyte(A), sigma=cs, low_threshold=ct[0], high_threshold=ct[1])\n",
    "  hough_radii = np.arange(hr[0], hr[1], 1) # the range of radii to use in search\n",
    "  hough_res = tf.hough_circle(edges, hough_radii) # look for circles\n",
    "  accums, cx, cy, radii = tf.hough_circle_peaks(hough_res, hough_radii, min_xdistance=hd, \n",
    "                                           min_ydistance=hd, total_num_peaks=hp, \n",
    "                                           threshold=ht, normalize=False)\n",
    "\n",
    "  # remove false positives (bright disks with dark perimeter)\n",
    "  pix = [] # as an empty list (for the remaining center pixels)\n",
    "  for center_y, center_x, radius in zip(cy, cx, radii):\n",
    "    c = circle(center_y, center_x, radius, shape=A0.shape) # central disk\n",
    "    cp = circle_perimeter(center_y, center_x, radius+1, shape=A0.shape) # perimeter ring\n",
    "    if (np.mean(A0[cp]) / np.mean(A0[c])) > cr:\n",
    "      pix.append((center_x, center_y)) # dark disks with bright perimeter are OK\n",
    "\n",
    "  # remove duplicates (close center pixels)\n",
    "  pix = np.array(pix) # as a numpy array\n",
    "  tree = cKDTree(pix) # for pairwise distance query\n",
    "  rows_to_fuse = list(tree.query_pairs(r=8.0))\n",
    "  p = np.ones(pix.shape[0])           # array of \"keep\" flags\n",
    "  p[np.array(rows_to_fuse)[:,0]] = 0  # flag the first of all duplicate pairs for deletion\n",
    "\n",
    "  # append to the landmark list\n",
    "  temp = np.full((np.count_nonzero(p),1),np.float(i))\n",
    "  pp = pix[p.astype(bool)].astype(float)\n",
    "  pp = np.concatenate((pp,temp),axis=1)\n",
    "  pp = list(map(tuple,pp)) # the remaining center pixels \n",
    "  pixx += pp \n",
    "  print(\"stack index\", i, \"-\", np.count_nonzero(p), \"nuclei identified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Plot all landmark nuclei centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f151f5f2444ad1891e25da3a9b8401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot landmarks\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "tp = np.array(pixx)\n",
    "ax.scatter(tp[:,0],tp[:,1],tp[:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify landmark \"threads\" to use for image stabilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef016b498b9490ca8a489a0470518a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of landmark threads: 114\n",
      "Number of deleted noise points: 1047\n"
     ]
    }
   ],
   "source": [
    "# identify and plot landmark threads\n",
    "\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# distance based spatial clustering\n",
    "tp = np.array(pixx)\n",
    "tpp = tp * [1.0,1.0,0.5] # compress the z scale\n",
    "db = DBSCAN(eps=10, min_samples=10).fit(tpp)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# get cluster and noise counts\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Number of landmark threads: %d' % n_clusters_)\n",
    "print('Number of deleted noise points: %d' % n_noise_)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "  class_member_mask = (labels == k)\n",
    "  xy = tp[class_member_mask & core_samples_mask]\n",
    "  ax.scatter(xy[:, 0], xy[:, 1], xy[:, 2], color=tuple(col))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlim3d(0,512)\n",
    "ax.set_ylim3d(0,512)\n",
    "\n",
    "# 5,9,11,12,15,18\n",
    "tpp = tp[labels==18]\n",
    "#print(tpp)\n",
    "ax.plot(tpp[:,0],tpp[:,1],tpp[:,2])\n",
    "\n",
    "#print(tpp)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tpp[:,0]\n",
    "y = tpp[:,1]\n",
    "z = tpp[:,2]\n",
    "\n",
    "tckp,u = splprep([x,y,z],s=2000,k=3,nest=-1)\n",
    "xnew,ynew,znew = splev(np.linspace(0,1,450),tckp)\n",
    "print(znew.shape)\n",
    "\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlim3d(0,512)\n",
    "ax.set_ylim3d(0,512)\n",
    "\n",
    "ax.plot(xnew,ynew,znew)\n",
    "#ax.scatter(x,y,z,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.zeros((images.shape[0], 512, 512, 3), dtype=np.uint8)\n",
    "for i in range(3, out.shape[0]-3):\n",
    "  tform = tf.AffineTransform(translation=(xnew[0]-xnew[i],ynew[0]-ynew[i]))\n",
    "  out[i] = color.gray2rgb(img_as_ubyte(tf.warp(images[i], tform.inverse)))\n",
    "  #out[i, int(ynew[0]),int(xnew[0])] = (255,0,0)\n",
    "  out[i][circle(int(ynew[0]),int(xnew[0]), 1.1, shape=out[i].shape)] = (255,0,0)\n",
    "io.imsave(\"warped-translation.tif\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = set(labels)\n",
    "ls.remove(-1)\n",
    "lxnew = []\n",
    "lynew = []\n",
    "for ll in ls:\n",
    "  f = (tp[labels==ll])[0,2]\n",
    "  l = (tp[labels==ll])[-1,2]\n",
    "  if f==3 and l==446:\n",
    "    tpp = tp[labels==ll]\n",
    "    tckp,u = splprep([tpp[:,0],tpp[:,1],tpp[:,2]],s=2000,k=3,nest=-1)\n",
    "    xnew,ynew,znew = splev(np.linspace(0,1,450),tckp)\n",
    "    lxnew.append(xnew)\n",
    "    lynew.append(ynew)\n",
    "lxnew = np.array(lxnew)\n",
    "lynew = np.array(lynew)\n",
    "lnew = np.array([lxnew, lynew])\n",
    "for i in range(3, out.shape[0]-3):\n",
    "  tform = tf.PiecewiseAffineTransform()\n",
    "  tform.estimate(np.transpose(lnew[:,:,i]), np.transpose(lnew[:,:,0]))\n",
    "  out[i] = color.gray2rgb(img_as_ubyte(tf.warp(images[i], tform.inverse)))\n",
    "  for p in np.transpose(lnew[:,:,0]):\n",
    "    #out[i, int(p[1]),int(p[0])] = (255,0,0)\n",
    "    out[i][circle(int(p[1]),int(p[0]), 1.1, shape=out[i].shape)] = (255,0,0)\n",
    "io.imsave(\"warped-hull.tif\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = set(labels)\n",
    "ls.remove(-1)\n",
    "lxnew = []\n",
    "lynew = []\n",
    "for ll in ls:\n",
    "  f = (tp[labels==ll])[0,2]\n",
    "  l = (tp[labels==ll])[-1,2]\n",
    "  if f==3 and l==446:\n",
    "    tpp = tp[labels==ll]\n",
    "    tckp,u = splprep([tpp[:,0],tpp[:,1],tpp[:,2]],s=2000,k=3,nest=-1)\n",
    "    xnew,ynew,znew = splev(np.linspace(0,1,450),tckp)\n",
    "    lxnew.append(xnew)\n",
    "    lynew.append(ynew)\n",
    "lxnew = np.array(lxnew)\n",
    "lynew = np.array(lynew)\n",
    "\n",
    "transx = np.mean(lxnew-lxnew[:,0][:,None], axis=0)\n",
    "transy = np.mean(lynew-lynew[:,0][:,None], axis=0)\n",
    "\n",
    "cornersx = np.full((lxnew.shape[1],4),[0,0,511,511]) + transx[:, None]\n",
    "cornersy = np.full((lynew.shape[1],4),[0,511,0,511]) + transy[:, None]\n",
    "\n",
    "lxnew = np.concatenate((lxnew, np.transpose(cornersx)))\n",
    "lynew = np.concatenate((lynew, np.transpose(cornersy)))\n",
    "lnew = np.array([lxnew, lynew])\n",
    "for i in range(3, out.shape[0]-3):\n",
    "  tform = tf.PiecewiseAffineTransform()\n",
    "  tform.estimate(np.transpose(lnew[:,:,i]), np.transpose(lnew[:,:,0]))\n",
    "  out[i] = color.gray2rgb(img_as_ubyte(tf.warp(images[i], tform.inverse)))\n",
    "  for p in np.transpose(lnew[:,:,0]):\n",
    "    #out[i, int(p[1]),int(p[0])] = (255,0,0)\n",
    "    out[i][circle(int(p[1]),int(p[0]), 1.1, shape=out[i].shape)] = (255,0,0)\n",
    "io.imsave(\"warped-full.tif\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stabilize the image stack using piece-wise affine transformation warping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 spanning threads.\n",
      "-3.0 29.0\n",
      "-5.0 10.0\n",
      "Warping frame: 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, DONE.\n"
     ]
    }
   ],
   "source": [
    "# get a copy of the original stack\n",
    "A = img_as_float(io.imread(\"../image_stacks/\" + image_stack + \".tif\")) # convert to float\n",
    "out = np.copy(A)\n",
    "\n",
    "# find threads that span the stack\n",
    "# NOTE: there are no threads in the first or last three frames, so skip those\n",
    "tcount = 0     # spanning thread count\n",
    "ls = set(labels)\n",
    "ls.remove(-1)\n",
    "lxnew = []\n",
    "lynew = []\n",
    "for ll in ls:\n",
    "  f = (tp[labels==ll])[0,2]\n",
    "  l = (tp[labels==ll])[-1,2]\n",
    "  if f==3 and l==(A.shape[0]-4): # NOTE: there are no threads in the first or last three frames\n",
    "    tcount = tcount + 1\n",
    "    tpp = tp[labels==ll]\n",
    "    tckp,u = splprep([tpp[:,0],tpp[:,1],tpp[:,2]],s=2000,k=3,nest=-1)\n",
    "    xnew,ynew,znew = splev(np.linspace(0,1,450),tckp)\n",
    "    lxnew.append(xnew)\n",
    "    lynew.append(ynew)\n",
    "lxnew = np.array(lxnew)\n",
    "lynew = np.array(lynew)\n",
    "print(\"Using \" + str(tcount) + \" spanning threads.\")\n",
    "\n",
    "# find the translation x and y ranges (for image trimming)\n",
    "transx_min = np.floor(np.min(lxnew-lxnew[:,0][:,None]))\n",
    "transx_max = np.ceil(np.max(lxnew-lxnew[:,0][:,None]))\n",
    "transy_min = np.floor(np.min(lynew-lynew[:,0][:,None]))\n",
    "transy_max = np.ceil(np.max(lynew-lynew[:,0][:,None]))\n",
    "print(transx_min, transx_max)\n",
    "print(transy_min, transy_max)\n",
    "\n",
    "# translate the frame corners using the average of the spanning thread translations\n",
    "transx = np.mean(lxnew-lxnew[:,0][:,None], axis=0)\n",
    "transy = np.mean(lynew-lynew[:,0][:,None], axis=0)\n",
    "cornersx = np.full((lxnew.shape[1],4),[0,0,511,511]) + transx[:, None]\n",
    "cornersy = np.full((lynew.shape[1],4),[0,511,0,511]) + transy[:, None]\n",
    "lxnew = np.concatenate((lxnew, np.transpose(cornersx)))\n",
    "lynew = np.concatenate((lynew, np.transpose(cornersy)))\n",
    "lnew = np.array([lxnew, lynew])\n",
    "\n",
    "# piece-wise affine transformation warping\n",
    "print(\"Warping frame:\", end = '')\n",
    "for i in range(3, out.shape[0]-3):\n",
    "  print(' ' + str(i) + ',', end = '')\n",
    "  tform = tf.PiecewiseAffineTransform()\n",
    "  tform.estimate(np.transpose(lnew[:,:,i]), np.transpose(lnew[:,:,0]))\n",
    "  out[i] = tf.warp(A[i], tform.inverse)\n",
    "print(\" DONE.\")\n",
    "\n",
    "# save the stabilized image stack\n",
    "for i in range(3): # duplicate the first and last three frames\n",
    "  out[i] = out[3]\n",
    "  out[-(1+i)] = out[-4]\n",
    "io.imsave(\"../image_stacks/\" + image_stack + \"_stab.tif\", \n",
    "    img_as_int(out[:,:,:-20]), check_contrast=False)  # out[x,y] goes to image(y,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = io.imread(image_stack)\n",
    "io.imsave(\"warped_ORIG.tif\", temp[:, 5:-5, 5:-30], check_contrast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange, cos, linspace, pi, sin, random\n",
    "from scipy.interpolate import splprep, splev\n",
    "\n",
    "# make ascending spiral in 3-space\n",
    "t=linspace(0,1.75*2*pi,100)\n",
    "\n",
    "x = sin(t)\n",
    "y = cos(t)\n",
    "z = t\n",
    "\n",
    "# add noise\n",
    "x+= random.normal(scale=0.1, size=x.shape)\n",
    "y+= random.normal(scale=0.1, size=y.shape)\n",
    "z+= random.normal(scale=0.1, size=z.shape)\n",
    "\n",
    "# spline parameters\n",
    "s=3.0 # smoothness parameter\n",
    "k=2 # spline order\n",
    "nest=-1 # estimate of number of knots needed (-1 = maximal)\n",
    "\n",
    "# find the knot points\n",
    "tckp,u = splprep([x,y,z],s=s,k=k,nest=-1)\n",
    "\n",
    "# evaluate spline, including interpolated points\n",
    "xnew,ynew,znew = splev(linspace(0,1,400),tckp)\n",
    "\n",
    "import pylab\n",
    "pylab.subplot(2,2,1)\n",
    "data,=pylab.plot(x,y,'bo-',label='data')\n",
    "fit,=pylab.plot(xnew,ynew,'r-',label='fit')\n",
    "pylab.legend()\n",
    "pylab.xlabel('x')\n",
    "pylab.ylabel('y')\n",
    "\n",
    "pylab.subplot(2,2,2)\n",
    "data,=pylab.plot(x,z,'bo-',label='data')\n",
    "fit,=pylab.plot(xnew,znew,'r-',label='fit')\n",
    "pylab.legend()\n",
    "pylab.xlabel('x')\n",
    "pylab.ylabel('z')\n",
    "\n",
    "pylab.subplot(2,2,3)\n",
    "data,=pylab.plot(y,z,'bo-',label='data')\n",
    "fit,=pylab.plot(ynew,znew,'r-',label='fit')\n",
    "pylab.legend()\n",
    "pylab.xlabel('y')\n",
    "pylab.ylabel('z')\n",
    "\n",
    "pylab.savefig('splprep_demo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/Borda/BIRL/blob/master/bm_experiments/bm_comp_perform.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from skimage import data, io\n",
    "from skimage.transform import resize, warp, AffineTransform\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import ransac\n",
    "from skimage.util import random_noise\n",
    "from skimage.restoration import denoise_bilateral, denoise_wavelet\n",
    "from skimage.feature import ORB, match_descriptors\n",
    "\n",
    "def register_image_pair(idx, path_img_target, path_img_source, path_out):\n",
    "    \"\"\" register two images together\n",
    "\n",
    "    :param int idx: empty parameter for using the function in parallel\n",
    "    :param str path_img_target: path to the target image\n",
    "    :param str path_img_source: path to the source image\n",
    "    :param str path_out: path for exporting the output\n",
    "    :return tuple(str,float):\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    # load and denoise reference image\n",
    "    img_target = 10.0*io.imread(path_img_target)[0]\n",
    "    img_target = denoise_wavelet(img_target, wavelet_levels=7, multichannel=False, rescale_sigma=False)\n",
    "\n",
    "    # load and denoise moving image\n",
    "    img_source = 10.0*io.imread(path_img_source)[50]\n",
    "    img_source = denoise_bilateral(img_source, sigma_color=0.05,\n",
    "                                   sigma_spatial=2, multichannel=False)\n",
    "\n",
    "    # detect ORB features on both images\n",
    "    detector_target = ORB(n_keypoints=150)\n",
    "    detector_source = ORB(n_keypoints=150)\n",
    "    detector_target.detect_and_extract(img_target)\n",
    "    detector_source.detect_and_extract(img_source)\n",
    "    matches = match_descriptors(detector_target.descriptors,\n",
    "                                detector_source.descriptors)\n",
    "    print(matches)\n",
    "    \n",
    "    # robustly estimate affine transform model with RANSAC\n",
    "    model, inliers = ransac((detector_target.keypoints[matches[:, 0]],\n",
    "                       detector_source.keypoints[matches[:, 1]]),\n",
    "                      AffineTransform, min_samples=25, max_trials=500,\n",
    "                      residual_threshold=0.95)  # 0.95\n",
    "    print(inliers)\n",
    "    \n",
    "    # warping source image with estimated transformations\n",
    "    img_warped = warp(img_target, model.inverse, output_shape=img_target.shape[:2])\n",
    "    path_img_warped = os.path.join(path_out, \"result.tif\")\n",
    "    io.imsave(path_img_warped, img_warped)\n",
    "\n",
    "    # summarise experiment\n",
    "    execution_time = time.time() - start\n",
    "    return path_img_warped, execution_time \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_image_pair(0, \n",
    "                    \"../image_stacks/Mistgcamp-3_0003.tif\", \n",
    "                    \"../image_stacks/Mistgcamp-3_0003.tif\", \n",
    "                    \"../image_stacks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The transforms module uses a (x, y) coordinate convention to be consistent\n",
    "#with most of the warping literature out there.  But the rest of\n",
    "#scikit-image uses  a (row, column) convention.\n",
    "\n",
    "#The following code works for me:\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "\n",
    "image = io.imread('image.jpg')\n",
    "h, w = image.shape[:2]\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "xs = rng.randint(0, w - 1, 76)\n",
    "ys = rng.randint(0, h - 1, 76)\n",
    "\n",
    "src_pts = np.column_stack([xs, ys])\n",
    "dst_pts = src_pts\n",
    "\n",
    "tform = transform.PiecewiseAffineTransform()\n",
    "tform.estimate(src_pts, dst_pts)\n",
    "\n",
    "out = transform.warp(image, tform)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(out)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
