{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python notebook for pre-processing an apical image stack.\n",
    "Assumes folder directory structure:\n",
    "<pre><code>  IMAGING\n",
    "    image_stacks\n",
    "    notebooks\n",
    "    results\n",
    "</code></pre>\n",
    "Execute the code sequentially, one block at a time, using &lt;shift-return&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy.interpolate import splprep, splev\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from skimage import color, data, exposure, filters, io\n",
    "from skimage.draw import circle, circle_perimeter\n",
    "from skimage.feature import canny\n",
    "from skimage.morphology import binary_erosion, binary_dilation\n",
    "from skimage.morphology import remove_small_objects\n",
    "from skimage.restoration import denoise_bilateral, denoise_wavelet\n",
    "from skimage.util import img_as_ubyte, img_as_int, img_as_float\n",
    "import skimage.transform as tf\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User editable parameters.\n",
    "Take care with formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "# user editable parameters\n",
    "\n",
    "image_stack = \"Movement2\"\n",
    "image_bits = 10\n",
    "\n",
    "# set to either True or False\n",
    "high_magnification = True \n",
    "\n",
    "################################################################################\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the image stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load picture\n",
    "images = io.imread(\"../image_stacks/\" + image_stack + \".tif\")\n",
    "images = np.float32(images/(2.0**image_bits))\n",
    "for i in images:\n",
    "  for l in range(i.shape[0] - 1): # moving average over every two lines\n",
    "    i[l] = (i[l] + i[l+1]) / 2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Use this code block to interactively explore landmark nuclei detection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6653b9349ee549d08aae86f053b4e5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='image gain', max=5.0, min=1.0), IntRangeSlider(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "@interact(\n",
    "  gn=widgets.FloatSlider(description='image gain',min=1.0, max=5.0, step=0.1, value=1.0),\n",
    "  sr=widgets.IntRangeSlider(description='stack range',min=0, max=300, step=1, value=[0,8]), \n",
    "  bs=widgets.FloatSlider(description='BILATERAL sigma',min=0.0, max=4.0, step=0.1, value=1.0), \n",
    "  cs=widgets.FloatSlider(description='CANNY sigma',min=1.0, max=4.0, step=0.1, value=1.8), \n",
    "  ct=widgets.IntRangeSlider(description='threshold',min=0, max=100, step=1, value=[9,22]),\n",
    "  hr=widgets.IntRangeSlider(description='HOUGH radii',min=3, max=25, step=1, value=[x*(2 if high_magnification else 1) for x in[5,8]]),\n",
    "  hd=widgets.IntSlider(description='distance',min=5, max=50, step=1, value=10),\n",
    "  hp=widgets.IntSlider(description='peaks',min=50, max=500, step=10, value=270),\n",
    "  ht=widgets.FloatSlider(description='threshold',min=0.0, max=1.0, step=0.01, value=0.12),\n",
    "  cr=widgets.FloatSlider(description='circle ratio',min=1.0, max=2.0, step=0.01, value=1.2))\n",
    "\n",
    "def f(gn, sr, bs, cs, ct, hr, hd, hp, ht, cr):\n",
    "  fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(8,8))\n",
    "  A = gn*np.mean(images[sr[0]:sr[1]], axis=0) # the static images\n",
    "  A0 = A / np.amax(A) # normalize\n",
    "  imageA = color.gray2rgb(img_as_ubyte(A0))\n",
    "\n",
    "  # identify nuclei (circles)   \n",
    "  #A = filters.gaussian(A0, sigma=gs) # noise filter\n",
    "  #A = denoise_wavelet(A0, wavelet_levels=7, multichannel=False, rescale_sigma=False)\n",
    "  A = denoise_bilateral(A0, sigma_spatial=bs)\n",
    "  edges = canny(img_as_ubyte(A), sigma=cs, low_threshold=ct[0], high_threshold=ct[1])\n",
    "  hough_radii = np.arange(hr[0], hr[1], 1) # the range of radii to use in search\n",
    "  hough_res = tf.hough_circle(edges, hough_radii) # look for circles\n",
    "  accums, cx, cy, radii = tf.hough_circle_peaks(hough_res, hough_radii, min_xdistance=hd, \n",
    "                                           min_ydistance=hd, total_num_peaks=hp, \n",
    "                                           threshold=ht, normalize=False)\n",
    "\n",
    "  # remove false positives (bright disks with dark perimeter)\n",
    "  pix = [] # as an empty list (for the remaining center pixels)\n",
    "  for center_y, center_x, radius in zip(cy, cx, radii):\n",
    "    c = circle(center_y, center_x, radius, shape=A0.shape) # central disk\n",
    "    cp = circle_perimeter(center_y, center_x, radius+1, shape=A0.shape) # perimeter ring\n",
    "    if (np.mean(imageA[cp]) / np.mean(imageA[c])) > cr:\n",
    "      pix.append((center_x, center_y)) # dark disks with bright perimeter are OK\n",
    "\n",
    "  # remove duplicates (close center pixels)\n",
    "  pix = np.array(pix) # as a numpy array\n",
    "  tree = cKDTree(pix) # for pairwise distance query\n",
    "  rows_to_fuse = list(tree.query_pairs(r=8.0))\n",
    "  p = np.ones(pix.shape[0])           # array of \"keep\" flags\n",
    "  p[np.array(rows_to_fuse)[:,0]] = 0  # flag the first of all duplicate pairs for deletion\n",
    "  pixx = pix[p.astype(bool)]          # the remaining center pixels \n",
    "\n",
    "  # draw nuclei centre pixels\n",
    "  for i in pixx:\n",
    "    #imageA[i[1], i[0]] = (255,0,0)\n",
    "    imageA[circle(i[1], i[0], 1.1, shape=A0.shape)] = (255,0,0)\n",
    "  \n",
    "  ax.imshow(imageA, norm=None)\n",
    "  plt.show()    \n",
    "  return(str(pixx.shape[0]) + \" nuclei identified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all landmark nuclei in the image stack.\n",
    "NOTE: Can take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame: 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, DONE.\n",
      "Range of per frame nuclei identified: 20-38\n"
     ]
    }
   ],
   "source": [
    "# landmark detection paramters\n",
    "bs = 1.0 \n",
    "cs = 1.8\n",
    "ct = [9,22]\n",
    "hr = [x*(2 if high_magnification else 1) for x in[5,8]]\n",
    "hd = 10\n",
    "hp = 270\n",
    "ht = 0.12\n",
    "cr = 1.2\n",
    "\n",
    "pixx = [] # a list of all the landmark nuclei centers\n",
    "min_n = 100000   # the least number of nuclei identified in a frame\n",
    "max_n = 0        # the most number of nuclei identified in a frame\n",
    "\n",
    "print(\"Processing frame: \", end = '')\n",
    "for i in range(3,images.shape[0]-3): # use moving average over seven frames\n",
    "  A = np.mean(images[i-3:i+5], axis=0)\n",
    "  A0 = A / np.amax(A) # normalized\n",
    "\n",
    "  # identify nuclei (circles)   \n",
    "  #A = filters.gaussian(A0, sigma=gs) # noise filter\n",
    "  #A = denoise_wavelet(A0, wavelet_levels=7, multichannel=False, rescale_sigma=False)\n",
    "  A = denoise_bilateral(A0, sigma_spatial=bs)\n",
    "  edges = canny(img_as_ubyte(A), sigma=cs, low_threshold=ct[0], high_threshold=ct[1])\n",
    "  hough_radii = np.arange(hr[0], hr[1], 1) # the range of radii to use in search\n",
    "  hough_res = tf.hough_circle(edges, hough_radii) # look for circles\n",
    "  accums, cx, cy, radii = tf.hough_circle_peaks(hough_res, hough_radii, min_xdistance=hd, \n",
    "                                           min_ydistance=hd, total_num_peaks=hp, \n",
    "                                           threshold=ht, normalize=False)\n",
    "\n",
    "  # remove false positives (bright disks with dark perimeter)\n",
    "  pix = [] # as an empty list (for the remaining center pixels)\n",
    "  for center_y, center_x, radius in zip(cy, cx, radii):\n",
    "    c = circle(center_y, center_x, radius, shape=A0.shape) # central disk\n",
    "    cp = circle_perimeter(center_y, center_x, radius+1, shape=A0.shape) # perimeter ring\n",
    "    if (np.mean(A0[cp]) / np.mean(A0[c])) > cr:\n",
    "      pix.append((center_x, center_y)) # dark disks with bright perimeter are OK\n",
    "\n",
    "  # remove duplicates (close center pixels)\n",
    "  pix = np.array(pix) # as a numpy array\n",
    "  tree = cKDTree(pix) # for pairwise distance query\n",
    "  rows_to_fuse = list(tree.query_pairs(r=8.0))\n",
    "  p = np.ones(pix.shape[0])           # array of \"keep\" flags\n",
    "  p[np.array(rows_to_fuse)[:,0]] = 0  # flag the first of all duplicate pairs for deletion\n",
    "\n",
    "  # get counts and append to the landmark list\n",
    "  temp = np.full((np.count_nonzero(p),1),np.float(i))\n",
    "  pp = pix[p.astype(bool)].astype(float)\n",
    "  pp = np.concatenate((pp,temp),axis=1)\n",
    "  pp = list(map(tuple,pp)) # the remaining center pixels \n",
    "  pixx += pp\n",
    "  c = np.count_nonzero(p)\n",
    "  if c < min_n:\n",
    "    min_n = c\n",
    "  if c > max_n:\n",
    "    max_n = c    \n",
    "  print(str(i) + \", \", end = '')\n",
    "print(\"DONE.\")\n",
    "print(\"Range of per frame nuclei identified:\", str(min_n) + '-' + str(max_n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Plot all landmark nuclei centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc83473e476f418cbaf9aa3b3cf736be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot landmarks\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"landmark nuclei centers\")\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "tp = np.array(pixx)\n",
    "ax.scatter(tp[:,0],tp[:,1],tp[:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify and plot landmark \"threads\" to use for image stabilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c9d278a6694ccf84323ea7ca082cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of landmark threads: 114\n",
      "Number of deleted noise points: 1047\n"
     ]
    }
   ],
   "source": [
    "# identify and plot landmark threads\n",
    "\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"landmark threads\")\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# distance based spatial clustering\n",
    "tp = np.array(pixx)\n",
    "tpp = tp * [1.0,1.0,0.5] # compress the z scale\n",
    "db = DBSCAN(eps=10, min_samples=10).fit(tpp)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# get cluster and noise counts\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Number of landmark threads: %d' % n_clusters_)\n",
    "print('Number of deleted noise points: %d' % n_noise_)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "  class_member_mask = (labels == k)\n",
    "  xy = tp[class_member_mask & core_samples_mask]\n",
    "  ax.scatter(xy[:, 0], xy[:, 1], xy[:, 2], color=tuple(col))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Identify and plot a sample thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b31c878cb54699a2dd21c63eeb8c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the longest thread\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "thread = unique[np.where(counts==np.max(counts[1:]))]\n",
    "\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"sample thread\")\n",
    "\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlim3d(0,images.shape[1])\n",
    "ax.set_ylim3d(0,images.shape[2])\n",
    "\n",
    "tpp = tp[labels==thread]\n",
    "ax.plot(tpp[:,0],tpp[:,1],tpp[:,2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Check thread smoothing parameters on the sample thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8054384c0068495583a2f6550f4209f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = tpp[:,0]\n",
    "y = tpp[:,1]\n",
    "z = tpp[:,2]\n",
    "\n",
    "# smooth the thread\n",
    "tckp,u = splprep([x,y,z],k=3,nest=-1,s=2000)\n",
    "xnew,ynew,znew = splev(np.linspace(0,1,450),tckp)\n",
    "\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"sample thread - smoothed\")\n",
    "\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlim3d(0,512)\n",
    "ax.set_ylim3d(0,512)\n",
    "\n",
    "ax.plot(xnew,ynew,znew)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stabilize the image stack using piece-wise affine transformation warping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 spanning threads.\n",
      "Warping frame: 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, DONE.\n"
     ]
    }
   ],
   "source": [
    "# get a copy of the original stack\n",
    "A = img_as_float(io.imread(\"../image_stacks/\" + image_stack + \".tif\")) # convert to float\n",
    "out = np.copy(A)\n",
    "\n",
    "# find threads that span the stack\n",
    "# NOTE: there are no threads in the first or last three frames, so skip those\n",
    "tcount = 0     # spanning thread count\n",
    "ls = set(labels)\n",
    "ls.remove(-1)\n",
    "lxnew = []\n",
    "lynew = []\n",
    "for ll in ls:\n",
    "  f = (tp[labels==ll])[0,2]\n",
    "  l = (tp[labels==ll])[-1,2]\n",
    "  if f==3 and l==(A.shape[0]-4): # NOTE: there are no threads in the first or last three frames\n",
    "    tcount = tcount + 1\n",
    "    tpp = tp[labels==ll]\n",
    "    tckp,u = splprep([tpp[:,0],tpp[:,1],tpp[:,2]],s=2000,k=3,nest=-1)\n",
    "    xnew,ynew,znew = splev(np.linspace(0,1,450),tckp)\n",
    "    lxnew.append(xnew)\n",
    "    lynew.append(ynew)\n",
    "lxnew = np.array(lxnew)\n",
    "lynew = np.array(lynew)\n",
    "print(\"Found \" + str(tcount) + \" spanning threads.\")\n",
    "\n",
    "# find image cropping values (to eliminate black borders caused by translation)\n",
    "XL = np.max(-np.int(np.floor(np.min(lxnew-lxnew[:,0][:,None]))),0)\n",
    "XR = np.min(-np.int(np.ceil(np.max(lxnew-lxnew[:,0][:,None]))),0)\n",
    "YL = np.max(-np.int(np.floor(np.min(lynew-lynew[:,0][:,None]))),0)\n",
    "YR = np.min(-np.int(np.ceil(np.max(lynew-lynew[:,0][:,None]))),0)\n",
    "\n",
    "# translate the frame corners using the average of the spanning thread translations\n",
    "transx = np.mean(lxnew-lxnew[:,0][:,None], axis=0)\n",
    "transy = np.mean(lynew-lynew[:,0][:,None], axis=0)\n",
    "cornersx = np.full((lxnew.shape[1],4),[0,0,511,511]) + transx[:, None]\n",
    "cornersy = np.full((lynew.shape[1],4),[0,511,0,511]) + transy[:, None]\n",
    "lxnew = np.concatenate((lxnew, np.transpose(cornersx)))\n",
    "lynew = np.concatenate((lynew, np.transpose(cornersy)))\n",
    "lnew = np.array([lxnew, lynew])\n",
    "\n",
    "# piece-wise affine transformation warping\n",
    "print(\"Warping frame:\", end = '')\n",
    "for i in range(3, out.shape[0]-3):\n",
    "  print(' ' + str(i) + ',', end = '')\n",
    "  tform = tf.PiecewiseAffineTransform()\n",
    "  tform.estimate(np.transpose(lnew[:,:,i]), np.transpose(lnew[:,:,0]))\n",
    "  out[i] = tf.warp(A[i], tform.inverse)\n",
    "print(\" DONE.\")\n",
    "\n",
    "# save the stabilized image stack\n",
    "for i in range(3): # duplicate the first and last three frames\n",
    "  out[i] = out[3]\n",
    "  out[-(1+i)] = out[-4]\n",
    "io.imsave(\"../image_stacks/\" + image_stack + \"_stab.tif\", \n",
    "    img_as_int(out[:,YL:YR,XL:XR]), check_contrast=False)  # out[x,y] goes to image(y,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Saved a cropped copy of the original image stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved cropped copy of original image\n",
    "A = io.imread(\"../image_stacks/\" + image_stack + \".tif\")\n",
    "io.imsave(\"../image_stacks/\" + image_stack + \"_orig.tif\", \n",
    "    A[:,yl:yr,xl:xr], check_contrast=False)  # out[x,y] goes to image(y,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
